2 AI 
video and computer games, sat ­nav systems, and Google’s search 
engine are all based on AI techniques. So are the systems used by financiers to predict movements on the stock market, and by national governments to help guide policy decisions in health and transport. So are the apps on mobile phones. Add avatars in virtual reality, and the toe ­in­the ­water models of emotion developed for 
“companion” robots. Even art galleries use AI —on their websites, 
and also in exhibitions of computer art. Less happily, military drones roam today’s battlefields—but, thankfully, robot mine­sweepers do so too.
AI has two main aims. One is technological: using computers to 
get useful things done (sometimes by employing methods very unlike those used by minds). The other is scientific: using AI concepts and models to help answer questions about human beings and other living things. Most AI workers focus on only one of these, but some consider both.
Besides providing countless technological gizmos, AI has deeply 
influenced the life sciences. A computer model of a scientific theory is a test of its clarity and coherence, and a compelling demonstration of its—often unknown—implications. Whether the theory is true  
is another matter, and depends on evidence drawn from the science concerned. But even discovering that it’s false can be illuminating.
In particular, AI has enabled psychologists and neuroscientists 
to develop powerful theories of the mind ­brain. These include 
models of how the physical brain works, and—a different, but equally important, question—just what it is that the brain is doing: what computational (psychological) questions it is answering, and what sorts of information processing enable it to do that. Many unanswered questions remain, for AI itself has taught us that our What is Artificial Intelligence? 3
minds are very much richer than psychologists had previously imagined.
Biologists, too, have used AI—in the form of “artificial life” 
(A­Life), which develops computer models of differing aspects of living organisms. This helps them to explain various types of animal behavior, the development of bodily form, biological evolution, and the nature of life itself.
Besides affecting the life sciences, AI has influenced philosophy. 
Many philosophers today base their accounts of mind on AI concepts. They use these to address, for instance, the notorious mind–body problem, the conundrum of free will, and the many puzzles regard­ing consciousness. However, these philosophical ideas are hugely controversial. And there are deep disagreements about whether any AI system could possess real intelligence, creativity, or life.
Last, but not least, AI has challenged the ways in which we think 
about humanity—and its future. Indeed, some people worry about whether we actually have a future, because they foresee AI surpassing human intelligence across the board. Although a few thinkers welcome this prospect, most dread it: what place will remain, they ask, for human dignity and responsibility?
All these issues are explored in the following chapters.
Virtual Machines
“To think about AI,” someone might say, “is to think about computers.” Well, yes and no. The computers, as such, aren’t the point. It’s what they do that matters. In other words, although AI needs physical machines (i.e. computers), it’s best thought of as 
using what computer scientists call virtual machines.4 AI 
A virtual machine isn’t a machine depicted in virtual reality, nor 
something like a simulated car engine used to train mechanics. Rather, it’s the information-processing system that the programmer has in mind when writing a program, and that people have in mind when using it.
As an analogy, think of an orchestra. The instruments have to 
work. Wood, metal, leather, and cat ­gut all have to follow the laws 
of physics if the music is to happen as it should. But the concert ­
goers aren’t focused on that. Rather, they’re interested in the music. Nor are they concerned with individual notes—still less, with the vibrations in the air that are causing the sound. They’re listening to the musical “shapes” made up by the notes: the melodies and harmonies, themes and variations, slurs and syncopation.
Where AI is concerned, the situation is similar. A word pro­
cessor, for example, is thought of by its designer, and experienced by its users, as dealing directly with words and paragraphs. But the program itself usually contains neither. (Some do, e.g. copyright notices, which can be easily inserted by the user.) And a neural network (see Chapter  4) is thought of as doing information  processing in parallel, even though it’s usually implemented in a 
(sequential) von Neumann computer.
That’s not to say that a virtual machine is just a convenient 
fiction, a thing merely of our imagination. Virtual machines are actual realities. They can make things happen, both inside the system and (if linked to physical devices such as cameras or robot hands) in the outside world. AI workers trying to discover what’s going wrong when a program does something unexpected only rarely consider hardware faults. Usually, they’re interested in the  events and causal interactions in the virtual machinery, or software.What is Artificial Intelligence? 5
Programming languages, too, are virtual machines (whose 
instruc  tions have to be translated into machine code before they 
can be run). Some are defined in terms of lower ­level program­
ming languages, so translation is required at several levels. They’re needed because most people can’t think about information  processing in the bit patterns used for machine code, and no one can think about complex processes at that hugely detailed level.
That’s not true only of programming languages. Virtual machines 
in general are comprised of patterns of activity (information pro­cessing) that exist at various levels. Moreover, it’s not true only of virtual machines running on computers. We’ll see in Chapter  6 that the human mind can be understood as the virtual machine—or 
rather, the set of mutually interacting virtual machines, running in parallel (and developed or learned at different times)—that is implemented in the brain.
Progress in AI requires progress in defining interesting/useful 
virtual machines. More physically powerful computers (larger, faster) are all very well. They may even be necessary for certain kinds of virtual machines to be implemented. But they can’t be exploited unless informationally powerful virtual machines can be run on them. (Similarly, progress in neuroscience requires better understanding of what psychological virtual machines are being implemented by the physical neurons: see Chapter 7.)
Different sorts of external ­world information are used. Every 
AI system needs input and output devices, if only a keyboard and a screen. Often, there are also special ­purpose sensors (perhaps 
cameras, or pressure ­sensitive whiskers) and/or effectors (perhaps 
sound synthesizers for music or speech, or robot hands). The AI program connects with—causes changes in—these computer ­
world interfaces as well as processing information internally.6 AI 
AI processing usually also involves internal input and output 
devices, enabling the various virtual machines within the whole system to interact with each other. For example, one part of a chess program may detect a possible threat by noticing something happening in another, and may then interface with yet another in searching for a blocking move.
The Major Types of AI
How the information is processed depends on the virtual machine involved. As we’ll see in later chapters, there are five major types, each including many variations. One is classical, or symbolic, AI—sometimes called GOFAI (Good Old ­Fashioned AI). Another 
is artificial neural networks, or connectionism. In addition, there are evolutionary programming; cellular automata; and dynamical systems.
Individual researchers often use only one method, but hybrid 
virtual machines also occur. For instance, a theory of human action that switches continually between symbolic and connectionist processing is mentioned in Chapter  4. (This explains why, and how, it is that someone may be distracted from following through on a planned task by noticing something unrelated to it in the environment.) And a sensorimotor device that combines “situated” robotics, neural networks, and evolutionary programming is described in Chapter 5. (This device helps a robot to find its way “home” by using a cardboard triangle as a landmark.)
Besides their practical applications, these approaches can illumi­
nate mind, behavior, and life. Neural networks are helpful for modeling aspects of the brain, and for doing pattern ­recognition What is Artificial Intelligence? 7
and learning. Classical AI (especially when combined with statis­tics) can model learning too, and also planning and reasoning. Evolutionary programming throws light on biological evolution and brain development. Cellular automata and dynamical systems can be used to model development in living organisms. Some methodologies are closer to biology than to psychology, and some are closer to non ­reflective behavior than to deliberative thought. 
To understand the full range of mentality, all of them will be needed—and probably more.
Many AI researchers don’t care about how minds work: they 
seek technological efficiency, not scientific understanding. Even if their techniques originated in psychology, they now bear scant relation to it. We’ll see, however, that progress in general ­purpose 
AI (artificial general intelligence, or AGI) will require deep under­standing of the computational architecture of minds.
AI Foreseen
AI was foreseen in the 1840s by Lady Ada Lovelace.1 More accu­
rately, she foresaw part  of it. She focused on symbols and logic, 
having no glimmering of neural networks, nor of evolutionary and dynamical AI. Nor did she have any leanings towards AI’s psychological aim, her interest being purely technological.
She said, for instance, that a machine “might compose elaborate 
and scientific pieces of music of any degree of complexity or extent,” and might also express “the great facts of the natural world” in enabling “a glorious epoch in the history of the sciences.” (So she wouldn’t have been surprised to see that, two centuries later, scien  tists are using “Big Data” and specially crafted programming 8 AI 
tricks to advance knowledge in genetics, pharmacology, epidemiology . . . the list is endless.)
The machine she had in mind was the Analytical Engine. This 
gears ­and ­cogwheels device (never fully built) had been designed 
by her close friend Charles Babbage in 1834. Despite being  dedicated to algebra and numbers, it was essentially equivalent to a general ­purpose digital computer.
Ada Lovelace recognized the potential generality of the Engine, 
its ability to process symbols representing “all subjects in the universe.” She also described various basics of modern program­ming: stored programs, hierarchically nested subroutines, address­ing, microprogramming, looping, conditionals, comments, and even bugs. But she said nothing about just how musical composition, or scientific reasoning, could be implemented on Babbage’s machine. AI was possible, yes—but how to achieve it was still a mystery.
How AI Began
That mystery was clarified a century later by Alan Turing. In 1936, Turing showed that every possible computation can in principle be performed by a mathematical system now called a universal Turing machine.
2 This imaginary system builds, and modifies, 
combinations of binary symbols ­­represented as “0” and “1.” After 
codebreaking at Bletchley Park during World War II, he spent the rest of the 1940s thinking about how the abstractly defined Turing machine could be approximated by a physical machine (he helped design the first modern computer, completed in Manchester in 1948), and how such a contraption could be induced to perform intelligently.What is Artificial Intelligence? 9
Unlike Ada Lovelace, Turing accepted both goals of AI. He wanted 
the new machines to do useful things normally said to require intelligence (perhaps by using highly unnatural techniques), and also to model the processes occurring in biologically based minds.
The 1950 paper in which he jokily proposed the Turing Test (see 
Chapter 6) was primarily intended as a manifesto for AI.
3 (A fuller 
version had been written soon after the war, but the Official Secrets Act prevented publication.) It identified key questions about the information processing involved in intelligence (game playing, perception, language, and learning), giving tantalizing hints about what had already been achieved. (Only “hints”, because the work at Bletchley Park was still top ­secret.) It even suggested 
computational approaches—such as neural networks and evolu­tionary computing—that became prominent only much later. But the mystery was still far from dispelled. These were highly general remarks: programmatic, not programs.
Turing’s conviction that AI must be somehow possible was 
bolstered in the early 1940s by the neurologist/psychiatrist Warren McCulloch and the mathematician Walter Pitts. In their paper “A Logical Calculus of the Ideas Immanent in Nervous Activity,”
4 they 
united Turing’s work with two other exciting items (both dating from the early twentieth century): Bertrand Russell’s proposi tional 
logic and Charles Sherrington’s theory of neural synapses.
The key point about propositional logic is that it’s binary. Every 
sentence (also called a proposition) is assumed to be either true or false. There’s no middle way, no recognition of uncertainty or probability. Only two “truth ­values” are allowed, namely true and false.
Moreover, complex propositions are built, and deductive argu­
ments are carried out, by using logical operators (such as and, or, 10 AI 
and if–then) whose meanings are defined in terms of the truth/
falsity of the component propositions. For instance, if two (or more) propositions are linked by and, it’s assumed that both/all of them are true. So “Mary married Tom and Flossie married Peter” is true if, and only if, both “Mary married Tom” and “Flossie married Peter” are true. If, in fact, Flossie did not marry Peter, then the complex proposition containing “and” is itself false.
Russell and Sherrington could be brought together by McCulloch 
and Pitts because they had both described binary systems. The true/false values of logic were mapped onto the on/off activity of brain cells and the 0/1 of individual states in Turing machines. Neurons were believed by Sherrington to be not only strictly on/off, but also to have fixed thresholds. So logic gates (computing and, or, and not) were defined as tiny neural nets, which could be 
interconnected to represent highly complex propositions. Any­thing that could be stated in propositional logic could be computed by some neural network, and by some Turing machine.
In brief, neurophysiology, logic, and computation were bundled 
together—and psychology came along too. McCulloch and Pitts believed (as many philosophers then did) that natural language boils down, in essence, to logic. So all reasoning and opinion, from scientific argument to schizophrenic delusions, was grist for their theoretical mill. They foresaw a time when, for the whole of psychology, “specification of the [neural] net would contribute all that could be achieved in that field.”
The core implication was clear: one and the same theoretical 
approach—namely, Turing computation— could be applied to 
human and machine intelligence. (The McCulloch/Pitts paper even influenced computer design. John von Neumann, then intending What is Artificial Intelligence? 11
to use decimal code, was alerted to it and switched to binary instead.)
Turing, of course, agreed. But he couldn’t take AI much further: 
the technology available was too primitive. In the mid ­1950s, 
however, more powerful and/or easily usable machines were developed. “Easily usable,” here, doesn’t mean that it was easier to push the computer’s buttons, or to wheel it across the room. Rather, it means that it was easier to define new virtual machines (e.g. programming languages), which could be more easily used to define higher ­level virtual machines (e.g. programs to do mathe­
matics, or planning).
Symbolic AI research, broadly in the spirit of Turing’s manifesto, 
commenced on both sides of the Atlantic. One late ­1950s land mark 
was Arthur Samuel’s checkers (draughts) player, which made news­paper headlines because it learned to beat Samuel himself.
5 That was 
an intimation that computers might one day develop superhuman intelligence, outstripping the capacities of their programmers.
The second such intimation also occurred in the late 1950s, 
when the Logic Theory Machine not only proved eighteen of Russell’s key logical theorems, but found a more elegant proof of one of them.
6 This was truly impressive. Whereas Samuel was only 
a mediocre checkers player, Russell was a world ­leading logician. 
(Russell himself was delighted by this achievement, but the Jour-
nal of Symbolic Logic refused to publish a paper with a computer program named as an author, especially as it hadn’t proved a new theorem.)
The Logic Theory Machine was soon outdone by the General 
Problem Solver (GPS)
7—“outdone” not in the sense that GPS could 
surpass yet more towering geniuses, but in the sense that it wasn’t 12 AI 
limited to only one field. As the name suggests, GPS could be applied to any problem that could be represented (as explained in Chapter 2) in terms of goals, sub ­goals, actions, and operators. It 
was up to the programmers to identify the goals, actions, and operators relevant for any specific field. But once that had been done, the reasoning could be left to the program.
GPS managed to solve the “missionaries ­and ­cannibals” problem, 
for example. (Three missionaries and three cannibals on one side of a river; a boat big enough for two people; how can everyone cross the river, without cannibals ever outnumbering missionaries?) That’s difficult even for humans, because it requires one to go backwards in order to go forwards. (Try it, using pennies!)
The Logic Theory Machine and GPS were early examples of 
GOFAI. They are now “old ­fashioned,” to be sure. But they were 
also “good,” for they pioneered the use of heuristics and planning—both of which are hugely important in AI today (see Chapter 2).
GOFAI wasn’t the only type of AI to be inspired by the “Logical 
Calculus” paper. Connectionism, too, was encouraged by it. In the 1950s, networks of McCulloch ­Pitts logical neurons, either purpose ­
built or simulated on digital computers, were used (by Albert Uttley, for instance
8) to model associative learning and conditioned 
reflexes. (Unlike today’s neural networks, these did local, not dis-tributed, processing: see Chapter 4.)
But early network modeling wasn’t wholly dominated by 
neuro ­ logic. The systems implemented (in analogue computers) 
by Raymond Beurle in the mid ­1950s were very different.
9 Instead 
of carefully designed networks of logic gates, he started from two ­
dimensional arrays of randomly connected, and varying ­threshold, 
units. He saw neural self­organization as due to dynamical waves What is Artificial Intelligence? 13
of activation—building, spreading, persisting, dying, and some­times interacting.
As Beurle realized, to say that psychological processes could be 
modeled by a logic ­chopping machine wasn’t to say that the brain 
actually is such a machine. McCulloch and Pitts had already pointed this out. Only four years after their first groundbreaking paper, they had published another one arguing that thermodynamics is closer than logic to the functioning of the brain.
10 Logic gave way 
to statistics, single units to collectivities, and deterministic purity to probabilistic noise.
In other words, they had described what’s now called distributed, 
error ­tolerant computing (see Chapter  4). They saw this new 
approach as an “extension” of their previous one, not a contradiction of it. But it was more biologically realistic.
Cybernetics
McCulloch’s influence on early AI went even further than GOFAI and connectionism. His knowledge of neurology as well as logic made him an inspiring leader in the budding cybernetics move­ment of the 1940s.
The cyberneticians focused on biological self­organization. This 
covered various kinds of adaptation and metabolism, including autonomous thought and motor behavior as well as (neuro)physiological regulation. Their central concept was “circular causa­tion,” or feedback. And a key concern was teleology, or purpo­siveness. These ideas were closely related, for feedback depended on goal differences: the current distance from the goal was used to guide the next step.14 AI 
Norbert Wiener (who designed anti ­ballistic missiles during the 
war) named the movement in 1948, defining it as “the study of control and communication in the animal and the machine.”
11 
Those cyberneticians who did computer modeling often drew inspiration from control engineering and analogue computers rather than logic and digital computing. However, the distinction wasn’t clear ­cut. For instance, goal differences were used both to 
control guided missiles and to direct symbolic problem solving. Moreover, Turing—the champion of classical AI—used dynamical equations (describing chemical diffusion) to define self­organizing systems in which novel structure, such as spots or segmentation, could emerge from a homogeneous origin (see Chapter 5).
12
Other early members of the movement included the experi­
mental psychologist Kenneth Craik; the mathematician John von Neumann; the neurologists William Grey Walter and William Ross Ashby; the engineer Oliver Selfridge; the psychiatrist and anthropologist Gregory Bateson; and the chemist and psychologist Gordon Pask.
13
Craik, who died (aged 31) in a cycling accident in 1943, before the 
advent of digital computers, referred to analogue computing in thinking about the nervous system. He described perception and motor action, and intelligence in general, as guided by feedback from “models” in the brain.
14 His concept of cerebral models, or 
representations, would later be hugely influential in AI.
Von Neumann had puzzled about self­organization throughout 
the 1930s, and was hugely excited by McCulloch and Pitts’ first paper. Besides changing his basic computer design from decimal to binary, he adapted their ideas to explain biological evolution and reproduction. He defined various cellular automata: systems What is Artificial Intelligence? 15
made of many computational units, whose changes follow simple rules depending on the current state of neighboring units.
15 Some 
of these could replicate others. He even defined a universal repli­cator, capable of copying anything—itself included. Replication errors, he said, could lead to evolution.
Cellular automata were specified by von Neumann in abstract 
informational terms. But they could be embodied in many ways, for example, as self­assembling robots, Turing’s chemical diffusion, Beurle’s physical waves, or—as soon became clear—DNA.
From the late 1940s on, Ashby developed the Homeostat, an 
electrochemical model of physiological homeostasis.
16 This intri­
guing machine could settle into an overall equilibrium state no matter what values were initially assigned to its 100 param­eters  (allowing almost 400,000 different starting conditions). It illustrated Ashby’s theory of dynamical adaptation—both inside the body (not least, the brain) and between the body and its external environment, in trial ­and ­error learning and adaptive 
behavior.
Grey Walter, too, was studying adaptive behavior—but in a very 
different way.
17 He built mini ­robots resembling tortoises, whose 
sensorimotor circuitry modeled Sherrington’s theory of neural reflexes. These pioneering situated robots displayed lifelike behaviors such as light ­seeking, obstacle ­avoidance, and asso­
ciative learning via conditioned reflexes. They were sufficiently intriguing to be exhibited to the general public at the Festival of Britain in 1951.
Ten years later, Selfridge (grandson of the founder of the London 
department store) used symbolic methods to implement an essentially parallel ­processing system called Pandemonium.
1816 AI 
This GOFAI program learned to recognize patterns by having 
many bottom ­level “demons,” each always looking out for one 
simple perceptual input, which passed their results on to higher ­
level demons. These weighed the features recognized so far for consistency (e.g. only two horizontal bars in an F), downplaying 
any features that didn’t fit. Confidence levels could vary, and they mattered: the demons that shouted loudest had the greatest effect. Finally, a master ­demon chose the most plausible pattern, given 
the (often conflicting) evidence available. This research soon influenced both connectionism and symbolic AI. (One very recent offshoot is the LIDA model of consciousness: see Chapter 6.)
Bateson had little interest in machines, but he based his 1960s 
theories of culture, alcoholism, and “double ­bind” schizophrenia 
on ideas about communication (i.e. feedback) picked up earlier at cybernetic meetings. And from the mid ­1950s on, Pask—described 
as “the genius of self­organizing systems” by McCulloch—used cybernetic and symbolic ideas in many different projects. These included interactive theater; intercommunicating musical robots; architecture that learned and adapted to its users’ goals; chemically self­organizing concepts; and teaching machines. The latter enabled people to take different routes through a complex knowledge representation, so were suitable for both step ­by­step and holistic 
cognitive styles (and varying tolerance of irrelevance) on the learner’s part.
In brief, all the main types of AI were being thought about, and 
even implemented, by the late 1960s —and in some cases, much 
earlier than that.
Most of the researchers concerned are widely revered today. But 
only Turing was a constant specter at the AI feast. For many years, What is Artificial Intelligence? 17
the others were remembered only by some subset of the research community. Grey Walter and Ashby, in particular, were nearly forgotten until the late 1980s, when they were lauded (alongside Turing) as grandfathers of A­Life. Pask had to wait even longer for  recognition. To understand why, one must know how the com  puter modelers became disunited.
How AI Divided
Before the 1960s, there was no clear distinction between people modeling language or logical thinking and people modeling purposive/adaptive motor behavior. Some individuals worked on both. (Donald Mackay even suggested building hybrid computers, combining neural networks with symbolic processing.) And all were mutually sympathetic. Researchers studying physiologi cal 
self­regulation saw themselves as engaged in the same overall enter­prise as their psychologically oriented colleagues. They all attended the same meetings: the interdisciplinary Macy seminars in the USA (chaired by McCulloch from 1946 to 1951), and London’s seminal conference on “The Mechanization of Thought Processes” (organized by Uttley in 1958).
19
From about 1960, however, an intellectual schism developed. 
Broadly speaking, those interested in life  stayed in cybernetics, and 
those interested in mind turned to symbolic computing. The network enthusiasts were interested in both brain and mind, of course. But they studied associative learning in general, not specific semantic content or reasoning, so fell within cybernetics rather than symbolic AI. Unfortunately, there was scant mutual respect between these increasingly separate sub ­groups.18 AI 
The emergence of distinct sociological coteries was inevitable. 
For the theoretical questions being asked—biological (of varying kinds) and psychological (also of varying kinds)—were different. So too were the technical skills involved: broadly defined, logic versus differential equations. Growing specialization made  communication increasingly difficult, and largely unprofitable. Highly eclectic conferences became a thing of the past.
Even so, the division needn’t have been so ill ­tempered. The bad 
feeling on the cybernetic/connectionist side began as a mixture of professional jealousy and righteous indignation. These were prompted by the huge initial success of symbolic computing, by the journalistic interest attending the provocative term “artificial intelligence” (coined by John McCarthy in 1956 to name what had  previously been called “computer simulation”), and by the arrogance—and unrealistic hype—expressed by some of the symbolists.
Members of the symbolist camp were initially less hostile, 
because they saw themselves as winning the AI competition. Indeed, they largely ignored the early network research, even though some of their leaders (Marvin Minsky, for instance) had started out in that area.
In 1958, however, an ambitious theory of neurodynamics—
defining parallel ­processing systems capable of self­organized 
learning from a random base (and error ­tolerant to boot)—was 
presented by Frank Rosenblatt and partially implemented in his photoelectric Perceptron machine.
20 Unlike Pandemonium, this 
didn’t need the input patterns to be pre ­analyzed by the pro­
grammer. This novel form of connectionism couldn’t be ignored by the symbolists. But it was soon contemptuously dismissed. As What is Artificial Intelligence? 19
explained in Chapter 4, Minsky (with Seymour Papert) launched a stinging critique in the 1960s claiming that perceptrons are incapable of computing some basic things.
21
Funding for neural ­network research dried up accordingly. This 
outcome, deliberately intended by the two attackers, deepened the antagonisms within AI.
To the general public, it now seemed that classical AI was the 
only game in town. Admittedly, Grey Walter’s tortoises had received great acclaim in the Festival of Britain. Rosenblatt’s Perceptron was hyped by the press in the late 1950s, as was Bernard Widrow’s pattern ­learning Adaline (based on signal ­processing). But the 
symbolists’ critique killed that interest stone dead. It was symbolic AI which dominated the media in the 1960s and 1970s (and which influenced the philosophy of mind as well).
That situation didn’t last. Neural networks—as “PDP systems” 
(doing parallel distributed processing)—burst onto the public stage again in 1986 (see Chapter  4). Most outsiders—and some insiders, who should have known better—thought of this approach as utterly new. It seduced the graduate students, and attracted enormous journalistic (and philosophical) attention. Now, it was the symbolic AI people whose noses were put out of joint. PDP was in fashion, and classical AI was widely said to have failed.
As for the other cyberneticians, they finally came in from the 
cold with the naming of A­Life in 1987. The journalists, and the graduate students, followed. Symbolic AI was challenged yet again.
In the twenty­first century, however, it has become clear that 
different questions require different types of answers—horses for courses. Although traces of the old animosities remain, there’s now room for respect, and even cooperation, between different 20 AI 
approaches. For instance, “deep learning” is sometimes used in powerful systems combining symbolic logic with multilayer prob­abilistic networks; and other hybrid approaches include ambitious models of consciousness (see Chapter 6).
Given the rich variety of virtual machines that constitute the 
human mind, one shouldn’t be too surprised.2
General Intelligence as the  
Holy Grail
State-of-the-art AI is a many-splendored thing. It offers a profu -
sion of virtual machines, doing many different kinds of informa -
tion processing. There’s no key secret here, no core technique unifying the field: AI practitioners work in highly diverse areas, sharing little in terms of goals and methods. This book can mention only very few of the recent advances. In short, AI’s methodological range is extraordinarily wide.
One could say that it’s been astonishingly successful. For its 
practical range, too, is extraordinarily wide. A host of AI applica -
tions exist, designed for countless specific tasks and used in almost every area of life, by laymen and professionals alike. Many out -
perform even the most expert humans. In that sense, progress has been spectacular.
But the AI pioneers weren’t aiming only for specialist systems. 
They were also hoping for systems with general intelligence. Each human-like capacity they modeled—vision, reasoning, language, 22 AI 
learning, and so on—would cover its entire range of challenges. Moreover, these capacities would be integrated when appropriate.
Judged by those criteria, progress has been far less impressive. 
John McCarthy recognized AI’s need for “common sense” very early on.
1 And he spoke on “Generality in Artificial Intelligence” in both 
of his high-visibility Turing Award addresses, in 1971 and 1987—but he was complaining, not celebrating. In 2016, his complaints aren’t yet answered.
The twenty-first century is seeing a revival of interest in artificial 
general intelligence (AGI), driven by recent increases in computer power. If that were achieved, AI systems could rely less on special purpose programming tricks, benefitting instead from general powers of reasoning and perception—plus language, creativity, and emotion (all of which are discussed in Chapter 3).
However, that’s easier said than done. General intelligence is still 
a major challenge, still highly elusive. AGI is the field’s Holy Grail.
Supercomputers aren’t Enough
Today’s supercomputers are certainly a help to anyone seeking to realize this dream. The combinatorial explosion—wherein more computations are required than can actually be executed—is no longer the constant threat that it used to be. Nevertheless, problems can’t always be solved merely by increasing computer power.
New problem-solving methods are often needed. Moreover, even 
if a particular method must succeed in principle, it may need too much time and/or memory to succeed in practice. Three such examples (concerning neural networks) are given in Chapter  4. General Intelligence as the Holy Grail 23
Similarly, a brute-force “solution” listing all possible chess moves would require more memory locations than there are electrons in the Universe—so even a large bunch of supercomputers wouldn’t suffice.
Efficiency is important, too: the fewer the number of compu -
tations, the better. In short, problems must be made tractable.
There are several basic strategies for doing that. All were 
pioneered by classical symbolic AI, or GOFAI, and all are still essential today.
One is to direct attention to only a part of the search space (the 
computer’s representation of the problem, within which the solution is assumed to be located). Another is to construct a smaller search space by making simplifying assumptions. A third is to order the search efficiently. Yet another is to construct a different search space, by representing the problem in a new way.
These approaches involve heuristics, planning, mathematical sim­
plification, and knowledge representation, respectively. The next five sections consider those general AI strategies.
Heuristic Search
The word “heuristic” has the same root as “Eureka!”: it comes from the Greek for find, or discover. Heuristics were highlighted by early 
GOFAI, and are often thought of as “programming tricks.” But the term didn’t originate with programming: it has long been familiar to logicians and mathematicians. As for the human activity of using heuristics in problem solving (whether self-consciously or not), this goes back thousands of years—long before AI was a twinkle in Ada Lovelace’s eye.24 AI 
Whether in humans or machines, heuristics make it easier to 
solve the problem. In AI, they do this by directing the program towards certain parts of the search space and away from others.
Many heuristics, including most of those used in the very early 
days of AI, are rules of thumb that aren’t guaranteed to succeed. The solution may lie in some part of the search space that the heuristic has led the system to ignore. For example, “Protect your queen” is a very helpful rule in chess, but it should occasionally be disobeyed.
Others can be logically or mathematically proved to be adequate. 
Much work in AI and computer science today aims to identify provable properties of programs. That’s one aspect of “Friendly AI,”
2 because human safety may be jeopardized by the use of 
logically unreliable systems (see Chapter 7). (There’s no principled distinction between heuristics and algorithms. Many algorithms are, in effect, mini-programs incorporating some particular heuristic.)
Whether reliable or not, heuristics are an essential aspect of AI 
research. The increasing AI specialism mentioned above depends partly on the definition of new heuristics that can improve effi-ciency spectacularly, but only in one highly restricted sort of problem, or search space. A hugely successful heuristic may not be suitable for “borrowing” by other AI programs.
Given several heuristics, their order of application may matter. 
For instance, “Protect your queen” should be taken into account before “Protect your bishop”—even though this ordering will occasionally lead to disaster. Different orderings will define different search trees through the search space. Defining and ordering heuristics are crucial tasks for modern AI. (Heuristics are prominent in cognitive psychology, too. Intriguing work on “fast General Intelligence as the Holy Grail 25
and frugal heuristics,” for example, indicates how evolution has equipped us with efficient ways of responding to the environment.
3)
Heuristics make brute-force search through the entire search 
space unnecessary. But they are sometimes combined with (limited) brute-force search. IBM’s chess program Deep Blue, which caused worldwide excitement by beating world champion Gary Kasparov in 1997, used dedicated hardware chips, processing 200 million positions per second, to generate every possible move for the next eight.
4
However, it had to use heuristics to select the “best” move within 
them. And since its heuristics weren’t reliable, even Deep Blue didn’t beat Kasparov every  time.
Planning
Planning,  too, is prominent in today’s AI—not least in a wide range 
of military activities.5 Indeed, the USA’s Department of Defense, 
which paid for the majority of AI research until very recently, has said that the money saved (by AI planning) on battlefield logistics in the first Iraq war outweighed all their previous investment.
Planning isn’t restricted to AI: we all do it. Think of packing for 
your holiday, for instance. You have to find all the things you want to take, which probably won’t all be found in the same place. You may have to buy some new items (sun cream, perhaps). You must decide whether to collect all the things together (perhaps on your bed, or on a table), or whether to put each one in your luggage when you find it. That decision will depend in part on whether you want to put the clothes in last of all, to prevent creasing. You’ll need a rucksack, or a suitcase, or maybe two: how do you decide?26 AI 
The GOFAI programmers who used planning as an AI technique 
had such consciously thought-out examples in mind. (The type of AI based on neural networks is very different, for it doesn’t try to mimic conscious deliberation: see Chapter 4.) That’s because the pioneers responsible for the Logic Theory Machine (see Chapter 1) and GPS were primarily interested in the psychology of human reasoning. They based their programs on experiments they’d done with human subjects, asked to “think aloud” so as to describe their own thought processes while doing logic puzzles.
Modern AI planners don’t rely so heavily on ideas garnered 
from conscious introspection or experimental observation. And their plans are very much more complex than was possible in the early days. But the basic idea is the same.
A plan specifies a sequence of actions, represented at a general 
level—a final goal, plus sub-goals and sub-sub-goals . . . —so that 
all details aren’t considered at once. Planning at a suitable level of abstraction can lead to tree pruning within the search space, so some details never need to be considered at all. Sometimes, the final goal is itself a plan of action—perhaps scheduling the deliveries to and from a factory or battlefield. At other times, it’s the answer to a question—for example, a medical diagnosis.
For any given goal, and expected situations, the planning pro -
gram needs: a list of actions—that is, symbolic operators—or action types (instantiated by filling in parameters derived from the pro -
blem), each of which can make some relevant change; for every action, a set of necessary prerequisites (cf. to grasp something, it must be within reach); and heuristics for prioritizing the required changes and ordering the actions. If the program decides on a particular action, it may have to set up a new sub-goal to satisfy General Intelligence as the Holy Grail 27
the prerequisites. This goal-formulating process can be repeated again and again.
Planning enables the program—and/or the human user—to 
discover what actions have already been taken, and why. The “why” refers to the goal hierarchy: this action was taken to satisfy that 
prerequisite, to achieve such ­and ­such a sub-goal. AI systems com -
monly employ techniques of “forward-chaining” and “backward- chaining,” which explain how the program found its solution. This helps the user to judge whether the action/advice of the program is appropriate.
Some current planners have tens of thousands of lines of code, 
defining hierarchical search spaces on numerous levels. These systems are often significantly different from the early planners.
For example, most don’t assume that all the sub-goals can be 
worked on independently (i.e. that problems are perfectly decom­posable). In real life, after all, the result of one goal-directed activity may be undone by another. Today’s planners can handle partially decomposable problems: they work on sub-goals independently, but can do extra processing to combine the resulting sub-plans if necessary.
The classical planners could tackle only problems in which the 
environment was fully observable, deterministic, finite, and static. But some modern planners can cope with environments that are partially observable (i.e. the system’s model of the world may be incomplete and/or incorrect) and probabilistic. In those cases, the system must monitor the changing situation during execution, so as to make changes in the plan—and/or in its own “beliefs” about the world—as appropriate. Some modern planners can do this over very long periods: they engage in continuous goal formulation, 28 AI 
execution, adjustment, and abandonment, according to the chang -
ing environment.
Many other developments have been added, and are still being 
added, to classical planning.6 It may seem surprising, then, that 
planning was roundly rejected by some roboticists in the 1980s, “situated” robotics being recommended instead (see Chapter  5). The notion of internal representation—of goals and possible actions, for example—was rejected as well. However, that criticism was largely mistaken. The critics’ own systems may not have represented goals, but they involved representations of other things, such as retinal stimulations and rewards. Moreover, even robotics, where this critique originated, often needs planning as well as purely reac  tive responses—to build soccer-playing robots, for instance.
7
Mathematical Simplification
Whereas heuristics leave the search space as it is (making the pro -
gram focus on only part of it), simplifying assumptions construct an unrealistic—but computationally tractable—search space.
Some such assumptions are mathematical. One example is the 
“i.i.d.” assumption, commonly used in machine learning. This represents the probabilities in the data as being much simpler than they actually are.
The advantage of mathematical simplification when defining the 
search space is that mathematical—that is, clearly definable and, to mathematicians at least, readily intelligible—methods of search can be used. But that’s not to say that any mathematically defined search will be useful. As noted above, a method that’s mathe -
matically guaranteed to solve every problem within a certain class 
may be unusable in real life, because it would need infinite time to General Intelligence as the Holy Grail 29
do so. It may, however, suggest approximations that are more practicable: see the discussion of “backprop” in Chapter 4.
Non-mathematical simplifying assumptions in AI are legion—
and often unspoken. One is the (tacit) assumption that problems can be defined and solved without taking emotions into account (see Chapter 3). Many others are built into the general knowledge representation that’s used in specifying the task.
Knowledge Representation
Often, the hardest part of AI problem solving is presenting the problem to the system in the first place. Even if it seems that someone can communicate directly with a program—by speaking in English to Siri, perhaps, or by typing French words into Google’s search engine—they can’t. Whether one’s dealing with texts or with images, the information (“knowledge”) concerned must be presented to the system in a fashion that the machine can understand—in other words, that it can deal with. (Whether that is real understanding is discussed in Chapter 6.)
AI’s ways of doing this are highly diverse. Some are developments/
variations of general methods of knowledge representation intro -
duced in GOFAI. Others, increasingly, are highly specialized methods, tailor-made for a narrow class of problems–. There may be, for instance, a new way of representing X-ray images, or photo -
graphs of a certain class of cancerous cells, carefully tailored to enable some highly specific method of medical interpretation (so, no good for recognizing cats, or even CAT scans).
In the quest for AGI, the general methods are paramount. Initially 
inspired by psychological research on human cognition, these include: sets of IF–THEN rules; representations of individual 30 AI 
concepts; stereotyped action sequences; semantic networks; and inference by logic or probability.
Let’s consider each of these in turn. (Another form of knowledge 
representation, namely neural networks, is described in Chapter 4.)
Rule-Based Programs
In rule-based programming, a body of knowledge/belief is rep -
resented as a set of IF–THEN rules linking Conditions to Actions: IF this Condition is satisfied,  THEN take that Action. This form 
of knowledge representation draws on formal logic (Emil Post’s “production” systems). But the AI pioneers Allen Newell and Herbert Simon believed it to underlie human psychology in general.
Both Condition and Action may be complex, specifying a 
conjunction (or disjunction) of several—perhaps many—items. If several Conditions are satisfied simultaneously, the most inclusive conjunction is given priority. So “IF the goal is to cook roast beef and Yorkshire pudding” will take precedence over “IF the goal is to cook roast beef ”—and adding “and three veg” to the Condition 
will trump that.
Rule-based programs don’t specify the order of steps in advance. 
Rather, each rule lies in wait to be triggered by its Condition. Nevertheless, such systems can be used to do planning. If they couldn’t, they would be of limited use for AI. But they do it differently from how it’s done in the oldest, most familiar, form of programming (sometimes called “executive control”).
In programs with executive control (like GPS and the Logic 
Theory Machine: see Chapter 1), planning is represented explicitly. The programmer specifies a sequence of goal-seeking instructions General Intelligence as the Holy Grail 31
to be followed step by step, in strict temporal order: “Do this, then do that; then look to see whether X is true; if it is, do such ­and ­such; if 
not, do so ­and ­so.”
Sometimes, the “this” or the “so ­and ­so” is an explicit instruction 
to set up a goal or sub-goal. For instance, a robot with the goal of leaving the room may be instructed [sic] to set up the sub-goal of opening the door; next, if examining the current state of the door shows it to be closed, set up the sub-sub-goal of grasping the door handle. (A human toddler may need a sub-sub-sub-goal—namely, getting an adult to grasp the unreachable door handle; and the infant may need several goals at even lower levels in order to do that.)
A rule-based program, too, could work out how to escape 
from the room. However, the plan hierarchy would be repre -
sented not as a temporally ordered sequence of explicit steps, but as the logical structure implicit in the collection of IF–THEN rules that comprise the system. A Condition may require that such-and-such a goal has already been set up (IF you want to open the door, and you aren’t tall enough). Similarly, an Action can include the setting up of a new goal or sub-goal (THEN ask an adult). Lower levels will be activated automatically (IF you want to ask some one to do something, THEN set up the goal of moving near 
to them).
Of course, the programmer has to have included the relevant 
IF–THEN rules (in our example, rules dealing with doors and door handles). But he/she doesn’t need to have anticipated all the potential logical implications of those rules. (That’s a curse, as well as a blessing, because potential inconsistencies may remain undiscovered for quite a while.)32 AI 
The active goals/sub-goals are posted on a central “blackboard,” 
which is accessible to the whole system. The information dis played 
on the blackboard includes not only activated goals but also percep -
tual input, and other aspects of current processing. (That idea has influenced a leading neuropsychological theory of con sciousness, 
and an AI model of consciousness based on it: see Chapter 6.)
Rule-based programs were widely used for the pioneering 
“expert systems” of the early 1970s. These included MYCIN, which offered advice to human physicians on identifying infectious dis -
eases and on prescribing antibiotic drugs, and DENDRAL, which performed spectral analysis of molecules within a particular area of organic chemistry. MYCIN, for instance, did medical dia gnosis 
by matching symptoms and background bodily properties (Condi -
tions) to diagnostic conclusions and/or suggestions for further tests or medication (Actions). Such programs were AI’s first move away from the hope of generalism towards the practice of special ism. 
And they were the first step towards Ada Lovelace’s dream of machine-made science (see Chapter 1).
The rule-based form of knowledge representation enables pro -
grams to be built gradually, as the programmer—or perhaps an AGI system itself—learns more about the domain. A new rule can be added at any time. There’s no need to rewrite the program from scratch. However, there’s a catch. If the new rule isn’t logically consistent with the existing ones, the system won’t always do what it’s supposed to do. It may not even approximate what it’s supposed to do. When dealing with a small set of rules, such logical conflicts are easily avoided, but larger systems are less transparent.
In the 1970s, the new IF–THEN rules were drawn from ongoing 
conversations with human experts, asked to explain their decisions. General Intelligence as the Holy Grail 33
Today, many of the rules don’t come from conscious introspection. But they are even more efficient. Modern expert systems (a term rarely used today) range from huge programs used in scientific research and commerce to humble apps on phones. Many outper -
form their predecessors because they benefit from additional forms of knowledge representation, such as statistics and special-purpose visual recognition, and/or the use of Big Data (see Chapter 4).
These programs can assist, or even replace, human experts in 
narrowly restricted fields. Some surpass the world leaders in those fields. Almost forty years ago, a rule-based system could outdo the supreme human expert in the diagnosis of soybean diseases.
8 
Now, there are countless examples, used to aid human profes -
sionals working in science, medicine, law . . . and even dress design. 
(Which isn’t entirely good news: see Chapter 7.)
Frames, Word-Vectors, Scripts, Semantic Nets
Other commonly used methods of knowledge representation concern individual concepts, not entire domains (such as medical diagnosis or dress design).
For instance, one can tell a computer what a room is by specify-
ing a hierarchical data structure (sometimes called a “frame”). This represents a room as having floor, ceiling, walls, doors, windows, and 
furniture (bed, bath, dining table . . . ). Actual rooms have varying 
numbers of walls, doors, and windows, so “slots” in the frame allow specific numbers to be filled in—and provide default assignments too (four walls, one door, one window).
Such data structures can be used by the computer to find 
analogies, answer questions, engage in a conversation, or write or 34 AI 
understand a story. And they’re the basis of CYC: an ambitious—some would say vastly overambitious—attempt to represent all human knowledge.
Frames can be misleading, however. Default assignments, for 
instance, are problematic. (Some rooms have no window, and open-  
plan rooms have no door.) Worse: what of everyday concepts such as dropping, or spilling? Symbolic AI represents our common-sense 
knowledge of “naïve physics” by constructing frames coding such facts as that a physical object will drop if unsupported. But a helium balloon won’t. Allowing explicitly for such cases is a never-ending task.
In some applications using recent techniques for dealing with 
Big Data, a single concept may be represented as a cluster, or “cloud,” made up of hundreds or thousands of sometimes-associated con -
cepts, with the probabilities of the many paired associations being distinguished: see Chapter  3. Similarly, concepts can now be represented by “word-vectors” rather than words. Here, semantic features that contribute to, and connect, many different concepts are discovered by the (deep learning) system, and used to predict the following word—in machine translation, for instance.
9 How  ever, 
these representations aren’t yet as amenable for use in reason ing 
or conversation, as classical frames.
Some data structures (called “scripts”) denote familiar action 
sequences.10 For instance, putting a child to bed often involves 
tucking them up, reading a story, singing a lullaby, and switching on the night light. Such data structures can be used for question-answering, and also for suggesting questions. If a mother omits the 
night light, questions can arise about Why? and What hap pened next? General Intelligence as the Holy Grail 35
In other words, therein lies the seed of a story. Accordingly, this form of knowledge representation is used for automatic story-writing—and would be needed by “companion” computers capable of engaging in normal human conversation (see Chapter 3).
An alternative form of knowledge representation for concepts 
is semantic networks (these are localist networks: see Chapter 4). Pioneered by Ross Quillian in the 1960s as models of human asso -
ciative memory, several extensive examples (e.g. WordNet) are now available as public data resources. A semantic network links concepts by semantic relations such as synonymy, antonymy, sub ordination, 
super ­ordination, part–whole—and often also by associative linkages 
assimilating factual world-knowledge to semantics (see Chapter 3).
The network may represent words as well as concepts, by adding 
links coding for syllables, initial letters, phonetics, and homonyms. Such 
a network is used by Kim Binsted’s JAPE and Graeme Ritchie’s STAND UP, which generate jokes (of nine different types) based on puns, alliteration, and syllable-switching. For example: Q: What do you call a depressed train? A: A low­comotive; Q: What do you get if you 
mix a sheep with a kangaroo? A: A woolly jumper.
A caveat: semantic networks aren’t the same thing as neural 
networks. As we’ll see in Chapter  4, distributed neural networks represent knowledge in a very different way. There, individual concepts are represented not by a single node in a carefully defined associative net, but by the changing pattern of activity across an entire network. Such systems can tolerate conflicting evidence, so aren’t bedeviled by the problems of maintaining logical con -
sistency (to be described in the next section). But they can’t do precise inference. Nevertheless, they’re a sufficiently important 36 AI 
type of knowledge representation (and a sufficiently important basis for practical applications) to merit a separate chapter.
Logic and the Semantic Web
If one’s ultimate aim is AGI, logic seems highly appropriate as a knowledge representation. For logic is generally applicable. In principle, the same representation (the same logical symbolism) can be used for vision, learning, language, and so on, and for any integration thereof. Moreover, it provides powerful methods of theorem proving to handle the information.
That’s why the preferred mode of knowledge representation in 
early AI was the predicate calculus. This form of logic has more representational power than propositional logic, because it can “get inside” sentences to express their meaning. For example, con -
sider the sentence “This shop has a hat to fit everyone.” Predicate calculus can clearly distinguish these three possible meanings: “For every human individual, there exists in this shop some hat that will fit them”; “There exists in this shop a hat whose size can be varied so as to fit any human being”; and “In this shop there exists a hat [presumably folded up!] large enough to fit all human beings simultaneously.”
For many AI researchers, predicate logic is still the preferred 
approach. CYC’s frames, for example, are based in predicate logic. So are the natural language processing (NLP) representations in com -
positional semantics (see Chapter 3). Sometimes, predicate logic is extended so as to represent time, cause, or duty/morality. Of course, that depends on someone’s having developed those forms of modal logic—which isn’t easy.General Intelligence as the Holy Grail 37
However, logic has disadvantages, too. One involves the com -
binatorial explosion. AI’s widely used “resolution” method for logical theorem proving can get bogged down in drawing conclusions that are true but irrelevant. Heuristics exist for guiding, and restricting, the conclusions—and for deciding when to give up (which the Sorcerer’s Apprentice couldn’t do). But they aren’t foolproof.
Another is that resolution theorem proving assumes that not­
not­X implies X. That’s a familiar idea: in reductio ad absurdum arguments, one tries to find a contradiction between someone’s claim and their premises. If the domain being reasoned about is completely understood, that’s logically correct. But users of pro -
grams (such as many expert systems) with built-in resolution often assume that failure to find a contradiction implies that no con -
tradiction exists—so-called “negation by failure.” Usually, that’s a mistake. In real life, there’s a big difference between proving that something is false and failing to prove that it’s true (think of wondering whether or not your partner is cheating on you). That’s because much of the evidence (potential premises) is unknown.
A third disadvantage is that in classical (“monotonic”) logic, 
once something is proved to be true, it stays true. In practice, that’s not always so. One may accept X for good reason (perhaps it was a default assignment, or even a conclusion from careful argument and/or strong evidence), but it can turn out later that X is no longer true—or wasn’t true in the first place. If so, one must revise one’s beliefs accordingly. Given a logic-based knowledge representa -
tion, that’s easier said than done. Many researchers, inspired by McCarthy,
11 have tried to develop “non-monotonic” logics that 
can tolerate changing truth-values. Similarly, people have defined various “fuzzy” logics, where a statement can be labeled as 38 AI 
probable/improbable, or as unknown, rather than true/false. Even so, no reliable defense against monotonicity has been found.
AI researchers developing logic-based knowledge representation 
are increasingly seeking the ultimate atoms of knowledge, or meaning, in general. They aren’t the first: McCarthy and Hayes did so 
in “Some Philosophical Problems from an AI Standpoint.”
12 That 
paper addressed many familiar puzzles, from free will to counter -
factuals. These included questions about the basic ontology of the universe: states, events, properties, changes, actions . . . what?
Unless one is a metaphysician at heart (a rare human passion), 
why should one care? And why should these arcane questions be “increasingly” pursued today? Broadly, the answer is that trying to design AGI raises questions about what ontologies the knowledge representation can use. These questions arise also in designing the semantic web.
The semantic web isn’t the same as the World Wide Web—which 
we’ve had since the 1990s. For the semantic web isn’t even state of the art: it’s state of the future. If and when it exists, machine-driven associative search will be improved and supplemented by machine understanding. This will enable apps and browsers to access infor -
mation from anywhere on the Internet, and to integrate different items sensibly in reasoning about questions. That’s a tall order. Besides requiring huge engineering advances in hardware and communications infrastructure, this ambitious project (directed by Sir Tim Berners-Lee) needs to deepen the Web-roaming programs’ understanding of what they’re doing.
Search engines like Google’s, and NLP programs in general, can 
find associations between words and/or texts—but there’s no understanding there. Here, this isn’t a philosophical point (for General Intelligence as the Holy Grail 39
that, see Chapter 6), but an empirical one—and a further obstacle to achieving AGI. Despite some seductively deceptive examples—such as WATSON, Siri, and machine translation (all discussed in Chapter 3)—today’s computers don’t grasp the meaning of what they “read” or “say.”
One aspect of this lack of understanding is programs’ inability 
to communicate with (learn from) each other, because they use different forms of knowledge representation and/or different fundamental ontologies. If semantic web researchers can develop a highly general ontology, this Tower of Babel situation might be overcome. So, the metaphysical questions raised in 1960s AI are now important for highly practical reasons.
Computer Vision
Today’s computers don’t understand visual images as humans do, either. (Again, this is an empirical point: whether AGIs could have conscious visual phenomenology is discussed in Chapter 6.)
Since 1980, the various knowledge representations used for AI 
vision have drawn heavily on psychology—especially the theories of David Marr and James Gibson.
13 Marr focused on building 3D 
representations (by inverting the image-formation process), not on using them for action. Gibson stressed visual affordances for action: visual cues that suggest a pathway, or a weight-bearing bough—or even a friendly or hostile species-member. Despite such psychological influences, however, current visual programs are gravely limited.
14
Admittedly, computer vision has achieved remarkable feats: 
facial recognition with 98% success, for instance. Or reading cursive handwriting. Or noticing someone behaving suspiciously 40 AI 
(continually pausing by car doors) in parking lots. Or identifying certain diseased cells, better than human pathologists can. Faced with such successes, one’s mind is strongly tempted to boggle.
But the programs (many are neural networks: see Chapter  4) 
usually have to know exactly what they’re looking for: for example, a face not upside down, not in profile, not partly hidden behind something else, and (for 98% success) lit in a particular way.
That word “usually” is important. In 2012, Google’s Research 
Laboratory integrated 1,000 large (sixteen-core) computers to form a huge neural network, with over a billion connections. Equipped with deep learning, it was presented with 10 million random images from YouTube videos. It wasn’t told what to look for, and the images weren’t labeled. Nevertheless, after three days one unit (one artificial neuron) had learned to respond to images of a cat’s face, and another to human faces.
Impressive? Well, yes. Intriguing, too: the researchers were quick 
to recall the idea of “grandmother cells” in the brain. Ever since the 1920s, neuroscientists have differed over whether or not these exist.
15 
To say that they do is to say that there are cells in the brain (either single neurons or small groups of neurons) that become active when, and only when, a grandmother, or some other specific feature, is perceived. Apparently, something analogous was going on in Google’s cat-recognizing network. And although the cats’ faces had to be full on and the right way up, they could vary in size, or appear in different positions within the 200 x 200 array. A further study, which trained the system on carefully pre-selected (but unlabeled) images of human faces, including some in profile, resulted in a unit that could sometimes—only sometimes—discrim -
inate faces turned away from the viewer.General Intelligence as the Holy Grail 41
There will soon be many more—and even more impressive—
such achievements. Multilayer networks have already made huge advances in face-recognition, and can sometimes find the most salient part of an image and generate a verbal caption (e.g. “people shopping in an outdoor market”) to describe it.
16 The recently 
initiated Large Scale Visual Recognition Challenge is annually increas -
ing the number of visual categories that can be recognized, and decreasing the constraints on the images concerned (e.g. the number and occlusion of objects). However, these deep-learning systems will still share some of the weaknesses of their predecessors.
For instance, they—like the cat’s-face recognizer—will have no 
understanding of 3D space, no knowledge of what a “profile,” or occlusion, actually is. Even vision programs designed for robots provide only an inkling of such matters.
The Mars Rover robots, such as Opportunity and Curiosity (landed 
in 2004 and 2012 respectively), rely on special knowledge-representation tricks: heuristics tailored for the 3D problems they’re expected to face. They can’t do pathfinding or object manip -
ulation in the general case. Some robots simulate animate vision, wherein the body’s own movements provide useful information (because they change the visual input systematically). But even they can’t notice a possible pathway, or recognize that this unfamiliar thing could be picked up by their robot hand whereas that could not.
By the time this book is published, there may be some excep -
tions. But they too will have limits. For instance, they won’t under -
stand “I can’t pick that up,” because they won’t understand can and cannot. That’s because the requisite modal logic probably still won’t be available for their knowledge representation.42 AI 
Sometimes, vision can ignore 3D space—when reading hand -
writing, for instance. And in many highly restricted 2D tasks, computer vision makes fewer errors than humans. Indeed, it can sometimes employ highly unnatural techniques to analyze detailed patterns (in X-rays, for example) that no human eye could recognize. (Similarly, 3D computer vision often achieves remarkable results by very unnatural means.)
But even 2D computer vision is limited. Despite considerable 
research effort on analogical, or iconic, representations,
17 AI can’t 
reliably use diagrams in problem solving—as we do in geometrical reasoning, or in sketching abstract relationships on the back of an envelope. (Similarly, psychologists don’t yet understand just how we do those things.)
In short, most human visual achievements surpass today’s AI. 
Often, AI researchers aren’t clear about what questions to ask. For instance, think about folding a slippery satin dress neatly. No robot can do this (although some can be instructed, step by step, how to fold an oblong terry towel). Or consider putting on a T-shirt: the head must go in first, and not via a sleeve—but why ? 
Such topological problems hardly feature in AI.
None of this implies that human-level computer vision is 
impossible. But achieving it is much more difficult than most people believe.
That’s because characterizing it is hugely difficult. So this is a 
special case of the fact noted in Chapter 1: that AI has taught us that human minds are hugely richer, and more subtle, than psychologists previously imagined. Indeed, that is the main lesson to be learned from AI.General Intelligence as the Holy Grail 43
The Frame Problem
Finding an appropriate knowledge representation, in whatever domain, is difficult partly because of the need to avoid the frame problem. (Beware: although this problem arises when using frames as a knowledge representation for concepts, the meanings of “frame” here are different.)
As originally defined by McCarthy and Hayes,
18 the frame 
problem involves assuming (during planning by robots) that an action will cause only these changes, whereas it may cause those too. 
More generally, the frame problem arises whenever impli cations 
tacitly assumed by human thinkers are ignored by the computer because they haven’t been made explicit.
The classic case is the monkey and bananas problem, wherein 
the problem-solver (perhaps an AI planner for a robot) assumes that nothing relevant exists outside the frame (see Figure 1).
My own favorite example is: If a man of twenty can pick ten pounds 
of blackberries in an hour, and a woman of eighteen can pick eight, how many will they gather if they go blackberrying together? For sure, “eighteen” isn’t a plausible answer. It could be much more (because they’re both showing off) or, more probably, much less. This example was even more telling fifty years ago, when I first encountered it. But why is that? Just what kinds of knowledge are involved here? And could an AGI overcome what appear to be the plain arithmetical facts?
The frame problem arises because AI programs don’t have a 
human’s sense of relevance (see Chapter 3). It can be avoided if all possible consequences of every possible action are known. In some 44 AI 
technical/scientific areas, that’s so. (So AI scientists sometimes claim that the frame problem has been solved—or, if they’re especially careful, “more or less” solved.
19) In general, however, it 
isn’t. That’s a major reason why AI systems lack common sense.
In brief, the frame problem lurks all around us—and is a major 
obstacle in the quest for AGI.BOX
Figure 1 Monkey and bananas problem: how does the monkey get the 
bananas? (The usual approach to this problem assumes, though doesn’t explicitly state, that the relevant “world” is that shown inside the dotted-line frame. In other words, nothing exists outside this frame which causes significant changes in it on moving the box.)Reprinted from M. A. Boden, Artificial Intelligence and Natural Man (1977: 387).General Intelligence as the Holy Grail 45
Agents and Distributed Cognition
An AI agent  is a self-contained (“autonomous”) procedure, com -
parable sometimes to a knee-jerk reflex and sometimes to a mini-mind.
20 Phone apps or spelling correctors could be called agents, 
but usually aren’t—because agents normally cooperate. They use their highly limited intelligence in cooperation with—or anyway, alongside—others to produce results that they couldn’t achieve alone. The interaction between agents is as important as the individuals themselves.
Some agent systems are organized by hierarchical control: top 
dogs and underdogs, so to speak. But many exemplify distributed cognition. This involves cooperation without hierarchical com mand 
structure (hence the prevarication, above, between “in cooperation with” and “alongside”). There’s no central plan, no top-down influence, and no individual possessing all the relevant knowledge.
Naturally occurring examples of distributed cognition include 
ant trails, ship navigation, and human minds. Ant trails emerge from the behavior of many individual ants, automatically dropping (and following) chemicals as they walk. Similarly, navigation and maneuvering of ships results from the interlocking activities of many people: not even the captain has all the necessary knowledge, and some crew members have very little indeed. Even a single mind involves distributed cognition, for it integrates many cognitive, motivational, and emotional subsystems (see Chapters 4 and 6).
Artificial examples include neural networks (see Chapter  4); 
an anthropologist’s computer model of ship navigation,
21 and 
A-Life work on situated robotics, swarm intelligence, and swarm 46 AI 
robotics (see Chapter 5); symbolic AI models of financial markets (the agents being banks, hedge funds, and large shareholders); and the LIDA model of consciousness (see Chapter 6).
Awareness of distributed cognition also aids design in human–
computer interaction, such as collaborative workplaces and  com -
puter interfaces. That’s because (as Yvonne Rogers puts it) it clarifies “the complex interdependencies between people, artefacts, and technological systems that can often be overlooked when using traditional theories of cognition.”
Clearly, then, human-level AGI would involve distributed 
cognition.
Machine Learning
Human-level AGI would include machine learning, too.22 However, 
this needn’t be human ­like. The field originated from psychologists’ 
work on concept learning and reinforcement. However, it now depends on fearsomely mathematical techniques, because the knowledge representations used involve probability theory and statistics. (One might say that psychology has been left far behind. Certainly, some modern machine learning systems bear little or no similarity to what might plausibly be going on in human heads. However, the increasing use of Bayesian probability in this area of AI parallels recent theories in cognitive psychology and neuroscience.)
Today’s machine learning is hugely lucrative. It’s used for data 
mining—and, given supercomputers doing a million billion calcu lations per second, for processing Big Data (see Chapter 3).
Some machine learning uses neural networks. But much relies 
on symbolic AI, supplemented by powerful statistical algorithms. General Intelligence as the Holy Grail 47
In fact, the statistics really do the work, the GOFAI merely guiding the worker to the workplace. Accordingly, some professionals regard machine learning as computer science and/or statistics—not AI. However, there’s no clear boundary here.
(Some computer scientists deliberately reject McCarthy’s label 
“AI”, because of its problematic philosophical implications: see Chapter  6. And some avoid it because they disapprove of the experimental, relatively unsystematic, nature of much—though by no means all—AI research.)
Machine learning has three broad types: supervised, unsupervised, 
and reinforcement learning. (The distinctions originated in psychol -
ogy, and different neurophysiological mechanisms may be involved; reinforcement learning, across species, involves dopamine.)
In supervised learning, the programmer “trains” the system by 
defining a set of desired outcomes for a range of inputs (labeled examples and non-examples), and providing continual feedback about whether it has achieved them. The learning system generates hypotheses about the relevant features. Whenever it classifies incorrectly, it amends its hypothesis accordingly. Specific error messages are crucial (not merely feedback that it was mistaken).
In unsupervised learning, the user provides no desired outcomes or 
error messages. Learning is driven by the principle that co-occurring features engender expectations that they will co-occur in future. Unsupervised learning can be used to discover knowledge. The programmers needn’t know what patterns/clusters exist in the data: the system finds them for itself.
Finally, reinforcement learning is driven by analogues of reward 
and punishment: feedback messages telling the system that what it just did was good or bad. Often, reinforcement isn’t simply binary, 48 AI 
but represented by numbers—like the scores in a video game. “What it just did” may be a single decision (such as a move in a game), or a series of decisions (e.g. chess moves culminating in checkmate). In some video games, the numerical score is updated at every move. In highly complex situations, such as chess, success (or failure) is signaled only after many decisions, and some procedure for credit assignment identifies the decisions most likely to lead to success. (Evolutionary AI is a form of reinforcement learning in which success is monitored by the fitness function: see Chapter 5.)
Symbolic machine learning in general assumes—what’s not 
obviously true—that the knowledge representation for learning must involve some form of probability distribution. And many learning algorithms assume—what is usually false—that every variable in the data has the same probability distribution, and all are mutually independent. That’s because this i.i.d. (independent and identically distributed) assumption underlies many mathe -
matical theories of probability, on which the algorithms are based. The mathematicians adopted the i.i.d. assumption because it makes the mathematics simpler. Similarly, using i.i.d. in AI simplifies the search space, thus making problem solving easier.
Bayesian statistics, however, deals with conditional probabilities, 
where items/events are not independent. Here, probability depends on distributional evidence about the domain. Besides being more realistic, this form of knowledge representation allows proba -
bilities to be changed if new evidence comes in. Bayesian tech -
niques are becoming increasingly prominent in AI—and in psy chology and neuroscience too. Theories of “the Bayesian brain” 
(see Chapter 4) capitalize on the use of non-i.i.d. evidence to drive, General Intelligence as the Holy Grail 49
and to fine-tune, unsupervised learning in perception and motor control.
Given various theories of probability, there are many different 
algorithms suitable for distinct types of learning and different data sets. For instance, Support Vector Machines—which accept the i.i.d. assumption—are widely used for supervised learning, especially if the user lacks specialized prior knowledge about the domain. “Bag of Words” algorithms are useful when the order of features can be ignored (as in searches for words, not phrases). And if the i.i.d. assumption is dropped, Bayesian techniques (“Helmholtz machines”) can learn from distributional evidence.
Most machine learning professionals use off-the-shelf statistical 
methods. The originators of those methods are highly prized by the industry: Facebook recently employed the creator of Support Vector Machines, and in 2013/2014 Google hired several key instigators of deep learning.
23
Deep learning is a promising new advance based in multilayer 
networks (see Chapter 4), by which patterns in the input data are recognized at various hierarchical levels.
24 In other words, deep 
learning discovers a multilevel knowledge representation—for 
instance, pixels to contrast detectors, to edge detectors, to shape detectors, to object parts, to objects.
One example is the cat’s-face detector that emerged from Google’s 
research on YouTube. Another, recently reported in Nature, is a reinforcement learner (the “DQN” algorithm) that has learned to play the classic Atari 2600 2D games.
25 Despite being given only pixels and 
game scores as input (and already knowing only the number of actions available for each game), this surpasses 75% of humans on 29 of the 49 games, and outperforms professional game testers on 22.50 AI 
It remains to be seen how far this achievement can be extended. 
Although DQN sometimes finds the optimal strategy, involving temporally ordered actions, it can’t master games whose planning encompasses a longer period of time.
Future neuroscience may suggest improvements to this system. 
The current version is inspired by the Hubel–Wiesel vision receptors—cells in visual cortex that respond only to movement, or only to lines of a particular orientation. (That’s no big deal: the  Hubel–Wiesel receptors inspired Pandemonium, too: see Chapter 1.) More unusually, this version of DQN is inspired also by the “experience replay” happening in the hippocampus during sleep. Like the hippocampus, the DQN system stores a pool of past samples, or experiences, and reactivates them rapidly during learning. This feature is crucial: the designers reported “a severe deterioration” in performance when it was disabled.
Generalist Systems
The Atari game player caused excitement—and merited publi -
cation in Nature—partly because it seemed to be a step towards AGI. A single algorithm, using no handcrafted knowledge representa -
tion, learned a wide range of competences on a variety of tasks involving relatively high-dimensional sensory input. No previous program had done that.
However (as remarked at the outset of this chapter), full AGI would 
do very much more. Difficult though it is to build a high-performing AI specialist, building an AI generalist is orders of magnitude harder. (Deep learning isn’t the answer: its aficionados admit that “new paradigms are needed” to combine it with complex reasoning—General Intelligence as the Holy Grail 51
scholarly code for “we haven”t got a clue”.)26 That’s why most AI 
researchers abandoned that early hope, turning instead to multifarious narrowly defined tasks—often with spectacular success.
AGI pioneers who retained their ambitious hopes included 
Newell and John Anderson. They originated SOAR and ACT-R respectively: systems begun in the early 1980s, and both still being developed (and used) some three decades later. However, they oversimplified the task, focusing on only a small subset of human competences.
In 1962, Newell’s colleague Simon had considered the zig-zagging 
path of an ant on uneven ground. Every movement, he said, is a direct reaction to the situation perceived by the ant at that moment (this is the key idea of situated robotics: see Chapter 5). Ten years later, Newell and Simon’s book Human Problem Solving described our intelligence as similar.
27 According to their psychological theory, 
perception and motor action are supplemented by internal rep -
resentations (IF–THEN rules, or “productions”) stored in memory, or newly built during problem solving.
“Human beings, viewed as behaving systems,” they said, “are 
quite simple.” But the emergent behavioral complexities are signif-icant. For instance, they showed that a system of only fourteen IF–THEN rules can solve cryptarithmetic problems (e.g. map the letters to the digits 0 to 9 in this sum: DONALD + GERALD = ROBERT, where D = 5). Some rules deal with goal/sub-goal organ -
ization. Some direct attention (to a specific letter or column). Some recall previous steps (intermediate results). Some recognize false starts. And others backtrack to recover from them.
Cryptarithmetic, they argued, exemplifies the computational 
architecture of all intelligent behavior—so this psychological 52 AI 
approach suited a generalist AI. From 1980, Newell (with John Laird and Paul Rosenbloom) developed SOAR (Success Oriented Achievement Realized). This was intended as a model of cognition as a whole.
28 Its reasoning integrated perception, attention, memory, 
association, inference, analogy, and learning. Ant-like (situated) responses were combined with internal deliberation. Indeed, delib  eration often resulted in reflex responses, because a previously 
used sequence of sub-goals could be “chunked” into one rule.
In fact, SOAR failed to model all aspects of cognition, and was 
later extended as people recognized some of the gaps. Today’s version is used for many purposes, from medical diagnosis to factory scheduling.
Anderson’s ACT-R family (Adaptive Control of Thought) are 
hybrid systems (see Chapter  4), developed by combining pro -
duction systems and semantic networks.
29 These programs, which 
recognize the statistical probabilities in the environment, model associative memory, pattern recognition, meaning, language, pro -
blem solving, learning, imagery, and (since 2005) perceptuo-motor control. ACT-R is primarily an exercise in scientific AI. Whereas commercial machine learning has forgotten its psychological roots, ACT-R is still deepening them (recently including neuro -
science too: e.g. sets of IF–THEN rules paralleling “modular” brain systems).
A key feature of ACT-R is the integration of procedural and 
declarative knowledge. Someone may know that a theorem of 
Euclid’s is true, without knowing how to use it in a geometrical proof. ACT-R can learn how to apply a propositional truth, by construct -
ing hundreds of new productions that control its use in many different circumstances. It learns which goals, sub-goals, and General Intelligence as the Holy Grail 53
sub-sub-goals . . . are relevant in which conditions, and what results 
a particular action will have in various circumstances. In short, it learns by doing. And (like SOAR) it can chunk several rules that are often carried out sequentially into a single rule. This parallels the difference between how human experts and novices solve “the same” problem: unthinkingly or painstakingly.
ACT-R has diverse applications. Its mathematics tutors offer 
personalized feedback, including relevant domain knowledge, and the goal/sub-goal structure of problem solving. Thanks to chunking, the grain size of their suggestions changes as the student’s learning proceeds. Other applications concern NLP; human–computer interaction; human memory and attention; driving and flying; and visual web search.
SOAR and ACT were contemporaries of another early attempt 
at AGI: Douglas Lenat’s CYC. This symbolic-AI system was launched in 1984, and is still under continuous development.
30
By 2015, CYC contained 62,000 “relationships” capable of linking 
the concepts in its database, and millions of links between those concepts. These include the semantic and factual associations stored in large semantic nets (see Chapter 3), and countless facts of naïve physics—the unformalized knowledge of physical phenomena (such as dropping and spilling) that all humans have. The system uses both monotonic and non-monotonic logics, and probabilities too, to reason about its data. (At present, all the concepts and links are hand-coded, but Bayesian learning is being added; this will enable CYC to learn from the Internet.)
It has been used by several US government agencies, including 
the Department of Defense (to monitor terrorist groups, for instance) and the National Institutes of Health, and by some major 54 AI 
banks and insurance companies. A smaller version—OpenCyc—has been publicly released as a background source for a variety of applications, and a fuller abridgment (ResearchCyc) is available for AI workers. Although OpenCyc is regularly updated (most recently in 2014), it contains only a small subset of CYC’s database, and a small subset of inference rules. Eventually, the complete (or near-complete) system will be commercially available. However, that could fall into malicious hands—unless specific measures are taken to prevent this (see Chapter 7).
CYC was described by Lenat in AI Magazine (1986) as “Using 
Common Sense Knowledge to Overcome Brittleness and Knowledge Acquisition Bottlenecks.” That is, it was specifically addressing McCarthy’s prescient challenge. Today, it’s the leader in modeling “common-sense” reasoning, and also in “understanding” the  concepts it deals with (which even apparently impressive NLP pro grams cannot do: see Chapter 3).
Nevertheless, it has many weaknesses. For example, it doesn’t 
cope well with metaphor (although the database includes many dead metaphors, of course). It ignores various aspects of naïve physics. Its NLP, although constantly improving, is very limited. And it doesn’t yet include vision. In sum, despite its en-CYC-lopedic aims, it doesn’t really encompass human knowledge.
The Dream Revitalized
Newell, Anderson, and Lenat beavered away in the background for 30 years. Recently, however, interest in AGI has revived markedly. An annual conference was started in 2008, and SOAR, ACT-R, and CYC are being joined by other supposedly generalist systems.General Intelligence as the Holy Grail 55
For instance, in 2010 the machine learning pioneer Tom Mitchell 
launched Carnegie Mellon’s NELL (Never-Ending Language Learner). This “common-sense” system builds its knowledge by trawling the Web non-stop (for five years at the time of writing) and by accepting online corrections from the public. It can make simple inferences based on its (unlabeled) data: for instance, the athlete Joe Bloggs plays tennis, since he’s on the Davis team. Starting with an ontology of 200 categories and relations (e.g. master, is due to), after five years it had enlarged the ontology and amassed 90 million candidate beliefs, each with its own confidence level.
The bad news is that NELL doesn’t know, for example, that you can 
pull objects with a string, but not push them. Indeed, the putative common sense of all AGI systems is gravely limited. Claims that the notorious frame problem has been “solved” are highly misleading.
NELL now has a sister program, NEIL: Never-Ending Image 
Learner. Some part-visual AGIs combine a logical-symbolic knowl -
edge representation with analogical, or graphical, representations (a distinction made years ago by Aaron Sloman, but still not well understood).
In addition, Stanford Research Institute’s CALO (Cognitive 
Assistant that Learns and Organizes) provided the spin-off Siri 
app (see Chapter  3), bought by Apple for $200 million in 2009. Comparable currently active projects include Stan Franklin’s intriguing LIDA (discussed in Chapter  6) and Ben Goertzel’s OpenCog, which learns its facts and concepts within a rich vir  tual 
world and also from other AGI systems. (LIDA is one of two gen -
eralist systems focused on consciousness; the other is CLARION.
31)
An even more recent AGI project, started in 2014, aims at 
developing “A Computational Architecture for Moral Competence 56 AI 
in Robots” (see Chapter 7). Besides the difficulties mentioned above, it will have to face the many problems that relate to morality.
A genuinely human-level system would do no less. No wonder, 
then, that AGI is proving so elusive.
Missing Dimensions
Nearly all of today’s generalist systems are focused on cognition. Anderson, for instance, aims to specify “how all the subfields in cognitive psychology interconnect.” (“All” the subfields? Although he addresses motor control, he doesn’t discuss touch or proprioception—which sometimes feature in robotics.) A truly general AI would cover motivation and emotion as well.
A few AI scientists have recognized this. Marvin Minsky and 
Sloman have both written insightfully about the computational archi tecture of whole minds, although neither has built a whole-
mind model.
32
Sloman’s MINDER model of anxiety is outlined in Chapter 3. His 
work (and Dietrich Dorner’s psychological theory) has inspired Joscha Bach’s MicroPsi: an AGI based on seven different “motives,” and using “emotional” dispositions in planning and action selec  tion. It has 
also influenced the LIDA system mentioned above (see Chapter 6).
But even these fall far short of true AGI. Minsky’s prescient AI 
manifesto, “Steps Toward Artificial Intelligence,” identified obsta  cles 
as well as promises.
33 Many of the former have yet to be over come. 
As Chapter 3 should help to show, human-level AGI isn’t within sight.
(Many AI professionals disagree. Some even add that AGI will 
soon become ASI—“S” for Superhuman—with Homo sapiens side -
lined accordingly: see Chapter 7.)